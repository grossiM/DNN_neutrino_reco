###################################################################################
# ---- ----  ---- ----  ---- ----  ---- ----  ---- ----  ---- ----  ---- ----  ---- ---- #
#                                                                                        #
#           template cfg file for the usage of the neural network                        #
#                                                                                        #
#                                                                                        #
# ---- ----  ---- ----  ---- ----  ---- ----  ---- ----  ---- ----  ---- ----  ---- ---- #
#REMEMBER TO CHECK THE RIGHT COMBINATION OF CONFIGURATION, i.e. unpol-->single data-train-input, training variables etc
[general]

[output]
#where are models stored
output-folder = /Users/it058990/Desktop/MicheleG/PhD/VBS_COST_Group/GITBicocca/DNN_storage/models/unpol_VBS_WWmuvWqq_phantom_1_6_nob_testscaler

save-steps = 0

[input]
data-train = /Users/it058990/Desktop/MicheleG/PhD/VBS_COST_Group/GITBicocca/DNN_storage/data/rootfiles/generation_phantom_1_6_NO_B/mu_ewk_semilept_unpol_nob_lhe1060439_train.h5

data-val = /Users/it058990/Desktop/MicheleG/PhD/VBS_COST_Group/GITBicocca/DNN_storage/data/rootfiles/generation_phantom_1_6_NO_B/mu_ewk_semilept_unpol_nob_lhe353479_val.h5

number-of-events = 200

[training]
#only leptonic variables
#training-variables = mu_px,mu_py,mu_pz,mu_E,v_mu_px,v_mu_py
training-variables = mu_px,mu_py,mu_pz,mu_E,v_mu_px,v_mu_py,q_fin_px1,q_fin_py1,q_fin_pz1,q_fin_E1,q_fin_px2,q_fin_py2,q_fin_pz2,q_fin_E2,q_fin_px3,q_fin_py3,q_fin_pz3,q_fin_E3,q_fin_px4,q_fin_py4,q_fin_pz4,q_fin_E4,event0_mass,event1_mass,mu_pt,mu_eta,mu_phi

solutions = v_mu_sol0,v_mu_sol1

discriminating-variable = truth_cos_theta
#discriminating-variable = v_mu_label

model = custom_model

neurons = 20,60,100,200
hidden-layers = 2,3,4,6,8,10

shape = f
first-neuron = 200
last-neuron = 20

#https://medium.com/the-theory-of-everything/understanding-activation-functions-in-neural-networks-9491262884e0
#activation = linear,sigmoid,tanh,elu,softmax
activation = relu
last_activation = linear
#do not change linear that is compulsory for regression

#https://heartbeat.fritz.ai/5-regression-loss-functions-all-machine-learners-should-know-4fb140e9d4b0
#loss = binary_crossentropy
loss = mean_squared_error
#metrics = binary_accuracy
metrics = mean_squared_error
optimizer = SGD  
#optimizer = adam
kernel_init = normal

epochs = 100
batch-size = 32,128
dropout-rate = 0.0
learning-rate =1e-3
decay-rate =0
patience = 0.0001:5
#grid-search = neurons,hidden-layers,batch-size
grid-search = neurons,hidden-layers,batch-size

[grid]
unfold = 1

[hardware]
#machine = (local/cluster)

#where put evaluated models, give a clear name
[evaluation]

output = /Users/it058990/Desktop/MicheleG/PhD/VBS_COST_Group/GITBicocca/DNN_storage/evaluation/unpol_VBS_WWmuvWqq_phantom_1_6_nob_17jan/ciao
#output = /root/Michele/DNN_storage/evaluation/unpol_VBS_WWmuvWqq_phantom_1_6_nob_17jan

model-of-interest = current_model_epoch100

binning =\-1,1,0.03
#model-pattern =
#type = binary 
type = regression

#data-eval = /Users/it058990/Desktop/MicheleG/PhD/VBS_COST_Group/GITBicocca/DNN_storage/data/rootfiles/generation_phantom_1_6_NO_B/mu_ewk_semilept_trans_nob_lhe539538_train.h5:/Users/it058990/Desktop/MicheleG/PhD/VBS_COST_Group/GITBicocca/DNN_storage/data/rootfiles/generation_phantom_1_6_NO_B/mu_ewk_semilept_long_nob_lhe528104_train.h5
data-eval = /Users/it058990/Desktop/MicheleG/PhD/VBS_COST_Group/GITBicocca/DNN_storage/data/rootfiles/generation_phantom_1_6_NO_B/mu_ewk_semilept_unpol_nob_lhe1060439_train.h5:/Users/it058990/Desktop/MicheleG/PhD/VBS_COST_Group/GITBicocca/DNN_storage/data/rootfiles/generation_phantom_1_6_NO_B/mu_ewk_semilept_nob_unpol_full_comp_lhe1070472_train.h5
