###################################################################################
# ---- ----  ---- ----  ---- ----  ---- ----  ---- ----  ---- ----  ---- ----  ---- ---- #
#                                                                                        #
#           template cfg file for the usage of the neural network                        #
#                                                                                        #
#                                                                                        #
# ---- ----  ---- ----  ---- ----  ---- ----  ---- ----  ---- ----  ---- ----  ---- ---- #
#REMEMBER TO CHECK THE RIGHT COMBINATION OF CONFIGURATION, i.e. unpol-->single data-train-input, training variables etc
[general]

################ IMPORTANT ################
scale-label = 1
################################################
[output]
#where are models stored
output-folder = /path/to/model
save-steps = 1

[input]
#change here only for training
data-train = /path/to/data/long_file.h5,/path/to/data/trans_file.h5

data-val = /path/to/data/long_val.h5,/path/to/data/trans_val.h5

number-of-events = -1

[training]
training-variables = mu_px,mu_py,mu_pz,mu_E,q_fin_px1,q_fin_py1,q_fin_pz1,q_fin_E1,q_fin_px2,q_fin_py2,q_fin_pz2,q_fin_E2,mu_pt,mu_eta,mu_phi,el_pt,el_eta,el_phi,pt_vv,pv_xx,pv_yy,mt2,p1x,p1y,p2x,p2y

#training-labels = el_truth_cos_theta,mu_truth_cos_theta
training-labels = v_mu_px,v_mu_py,v_mu_pz,v_el_px,v_el_py,v_el_pz
model = custom_model

neurons = 60
hidden-layers = 2,4
output-dim = 6

shape = f
first-neuron = 200
last-neuron = 20

#https://medium.com/the-theory-of-everything/understanding-activation-functions-in-neural-networks-9491262884e0
#activation = linear,sigmoid,tanh,elu,softmax
activation = relu
last_activation = linear
#do not change linear that is compulsory for regression

#https://heartbeat.fritz.ai/5-regression-loss-functions-all-machine-learners-should-know-4fb140e9d4b0
#loss = binary_crossentropy
loss = mean_squared_error
#metrics = binary_accuracy
metrics = mean_squared_error
optimizer = SGD  
#optimizer = adam
kernel_init = normal

epochs = 120
batch-size = 256
dropout-rate = 0.0
learning-rate =1e-3
#https://www.pyimagesearch.com/2019/08/05/keras-learning-rate-finder/
decay-rate =0
patience = 0.0001:5

grid-search = neurons,hidden-layers,batch-size

[grid]
unfold = 1

[hardware]
#machine = (local/cluster)

#######################################where put evaluated models, give a clear name
[evaluation]

output = /path/to/store/evaluatedfile

model-of-interest = current_model_epoch100

#type = binary 
type = regression

data-eval = /path/to/data/file.h5:/path/to/data/file.h5